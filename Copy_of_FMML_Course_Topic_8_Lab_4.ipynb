{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Srilathayama/FMML-LABS-/blob/main/Copy_of_FMML_Course_Topic_8_Lab_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puhRZ5-BnA9q"
      },
      "source": [
        "# Bayesian Machine Learning: Lab 4 - Probabilisitic Mixture Models"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LXzc1EbxgqfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Satkz5dl10oW"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/drive/1fxe8poXf3KUsoLLVOUy0vS7XlAfWlXgm?usp=sharing\"><img height=\"32px\" src=\"https://colab.research.google.com/img/colab_favicon.ico\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/Foundations-in-Modern-Machine-Learning/course-contents/tree/main/Bayesian/\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>\n",
        "<br>\n",
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxV4u4v8msS0",
        "outputId": "b2dca1cf-a552-43a1-c3d0-84abad8ef97b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import animation as anim\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as pg\n",
        "\n",
        "import tensorflow as tf\n",
        "import pymc3 as pm\n",
        "import sklearn, sklearn.cluster, sklearn.mixture\n",
        "import theano.tensor as tt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-8caebdc20d61>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpymc3\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmixture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pymc3'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozFCp36JnPDE"
      },
      "source": [
        "plt.style.use('ggplot')\n",
        "np.random.seed(1243451432)\n",
        "color_map = ['green', 'red', 'purple', 'blue', 'orange']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCtotysKURel"
      },
      "source": [
        "## Training Probabilistic Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24fWxdlBUUhd"
      },
      "source": [
        "### Let's some data for Unsupervised Clustering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3SlI2NGi0EI"
      },
      "source": [
        "y = np.random.choice([0, 1], p=[0.5, 0.5], size=1000)\n",
        "true_mu_1 = -5.0\n",
        "true_mu_2 = 5.0\n",
        "true_sigma_1 = 1.0\n",
        "true_sigma_2 = 2.0\n",
        "X = (y == 0) * np.random.normal(true_mu_1, true_sigma_1, size=len(y)) + (y == 1) * np.random.normal(true_mu_2, true_sigma_2, size=len(y))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(20, 10))\n",
        "sns.histplot(X[y == 0], ax=ax, color='#0000CC')\n",
        "sns.histplot(X[y == 1], ax=ax, color='#CC0000')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbWLK--YUfEN"
      },
      "source": [
        "### Expectation Maximization for 2 Classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VD50Sq_lsNk"
      },
      "source": [
        "from matplotlib import rc\n",
        "rc('animation', html='jshtml')\n",
        "\n",
        "mu, sigma = np.array([3, 6]), np.array([1, 1])\n",
        "z = np.random.random(size=len(X))\n",
        "fig, ax = plt.subplots(4, 2, figsize=(15 * 2, 6 * 4))\n",
        "ax = ax.reshape(-1)\n",
        "\n",
        "def gaussian(mu, sigma, X):\n",
        "    return np.exp(-0.5 * ((X - mu) / sigma) ** 2) / (np.sqrt(2 * np.pi) * sigma)\n",
        "\n",
        "def animate(i):\n",
        "    global z, mu, sigma, ax\n",
        "    ax[2 * i].cla()\n",
        "    domain = np.linspace(-10, 10, 100)\n",
        "    # Maximization Step\n",
        "    p0 = gaussian(mu[0], sigma[0], X)\n",
        "    p1 = gaussian(mu[1], sigma[1], X)\n",
        "    z = p1 / (p0 + p1)\n",
        "    ax[2 * i].plot(domain, 1000 * gaussian(mu[0], sigma[0], domain), color='r')\n",
        "    ax[2 * i].plot(domain, 1000 * gaussian(mu[1], sigma[1], domain), color='b')\n",
        "    # Plot stuff\n",
        "    sns.histplot(X[z <= 0.5], ax=ax[2 * i + 1], color='#0000CC')\n",
        "    sns.histplot(X[z > 0.5], ax=ax[2 * i + 1], color='#CC0000')\n",
        "    # Expectation Step\n",
        "    mu = np.array([np.sum((1 - z) * X) / np.sum(1 - z), np.sum(z * X) / np.sum(z)])\n",
        "    sigma = np.array([np.sqrt(np.sum((1 - z) * X ** 2) / np.sum(1 - z)), np.sqrt(np.sum((z) * X ** 2) / np.sum(z))])\n",
        "    print(mu, sigma)\n",
        "\n",
        "for i in range(4):\n",
        "    animate(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irxfLAyK7mkF"
      },
      "source": [
        "class GaussianMixture:\n",
        "\n",
        "    def __init__(self, n_clusters, true_mu, true_sigma):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.true_mu, self.true_sigma = true_mu, true_sigma \n",
        "        self.y = np.random.choice(list(range(n_clusters)), p=[1/n_clusters for _ in range(n_clusters)], size=1000)\n",
        "        self.X = np.sum(np.stack([(self.y == i) * np.random.normal(true_mu[i], true_sigma[i], size=len(y)) for i in range(n_clusters)], axis=0), axis=0)\n",
        "\n",
        "        self.mu, self.sigma = np.array([3, 6, 15]), np.array([1, 1, 0.7])\n",
        "        self.z = np.random.random((n_clusters, len(self.y)))\n",
        "        self.z = self.z / np.reshape(np.sum(self.z, axis=0), (1, -1))\n",
        "        self.colors = ['r', 'g', 'b', 'k', 'o', 'y'][:n_clusters]\n",
        "\n",
        "    @staticmethod\n",
        "    def gaussian(mu, sigma, X):\n",
        "        return np.exp(-0.5 * ((X - mu) / sigma) ** 2) / (np.sqrt(2 * np.pi) * sigma)\n",
        "\n",
        "    def expectation(self):\n",
        "        self.mu = np.array([np.sum(self.z[i] * self.X) / np.sum(self.z[i]) for i in range(self.n_clusters)])\n",
        "        self.sigma = np.array([np.sqrt(np.sum(self.z[i] * self.X ** 2) / np.sum(self.z[i])) for i in range(self.n_clusters)])\n",
        "\n",
        "    def maximization(self):\n",
        "        self.z = np.stack([gaussian(self.mu[i], self.sigma[i], self.X) for i in range(self.n_clusters)], axis=0)\n",
        "        self.z = self.z / np.reshape(np.sum(self.z, axis=0), (1, -1))\n",
        "\n",
        "    def plot_full(self, iterations):\n",
        "        z = np.random.random(size=len(self.X))\n",
        "        fig, ax = plt.subplots(iterations, 2, figsize=(15 * 2, 6 * iterations))\n",
        "        \n",
        "        if len(ax.shape) == 1:\n",
        "            ax = ax.reshape(1, ax.shape[0])\n",
        "\n",
        "        for iter in range(iterations):\n",
        "            ax[iter][0].cla()\n",
        "            domain = np.linspace(-10, 10, 100)\n",
        "            self.maximization()\n",
        "            for i in range(self.n_clusters):\n",
        "                ax[iter][0].plot(domain, gaussian(self.mu[i], self.sigma[i], domain), color=self.colors[i])\n",
        "            for i in range(self.n_clusters):\n",
        "                sns.histplot(self.X[self.z[i] > 0.5], ax=ax[iter][1], color=self.colors[i])\n",
        "            self.expectation()\n",
        "\n",
        "    def plot_truth():\n",
        "        fig, ax = plt.subplots(figsize=(20, 10))\n",
        "        for i in range(self.n_clusters):\n",
        "            sns.histplot(X[y == i], ax=ax, color=self.colors[i])\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "gmm = GaussianMixture(3, [-7.0, 0.0, 7.0], [1.0, 1.0, 2.0])\n",
        "gmm.plot_full(4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uUXf1FLHpPl"
      },
      "source": [
        "### The real math of Expectation Maximization (Optional)\n",
        "\n",
        "Remember, that EM algorithm is a **coordinate descent** optimization of variational lower bound $\\mathcal{L}(\\theta, q) = \\int q(T) \\log\\frac{p(X, T|\\theta)}{q(T)}dT\\to \\max$.\n",
        "\n",
        "<b>E-step</b>:<br>\n",
        "$\\mathcal{L}(\\theta, q) \\to \\max\\limits_{q} \\Leftrightarrow \\mathcal{KL} [q(T) \\,\\|\\, p(T|X, \\theta)] \\to \\min \\limits_{q\\in Q} \\Rightarrow q(T) = p(T|X, \\theta)$<br>\n",
        "<b>M-step</b>:<br> \n",
        "$\\mathcal{L}(\\theta, q) \\to \\max\\limits_{\\theta} \\Leftrightarrow \\mathbb{E}_{q(T)}\\log p(X,T | \\theta) \\to \\max\\limits_{\\theta}$\n",
        "\n",
        "For GMM, $\\theta$ is a set of parameters that consists of mean vectors $\\mu_c$, covariance matrices $\\Sigma_c$ and priors $\\pi_c$ for each component.\n",
        "\n",
        "Latent variables $T$ are indices of components to which each data point is assigned, i.e. $t_i$  is the cluster index for object $x_i$.\n",
        "\n",
        "The joint distribution can be written as follows: $\\log p(T, X \\mid \\theta) =  \\sum\\limits_{i=1}^N \\log p(t_i, x_i \\mid \\theta) = \\sum\\limits_{i=1}^N \\sum\\limits_{c=1}^C q(t_i = c) \\log \\left (\\pi_c \\, f_{\\!\\mathcal{N}}(x_i \\mid \\mu_c, \\Sigma_c)\\right)$,\n",
        "where $f_{\\!\\mathcal{N}}(x \\mid \\mu_c, \\Sigma_c) = \\frac{1}{\\sqrt{(2\\pi)^n|\\boldsymbol\\Sigma_c|}}\n",
        "\\exp\\left(-\\frac{1}{2}({x}-{\\mu_c})^T{\\boldsymbol\\Sigma_c}^{-1}({x}-{\\mu_c})\n",
        "\\right)$ is the probability density function (pdf) of the normal distribution $\\mathcal{N}(x_i \\mid \\mu_c, \\Sigma_c)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5jK14QL6E70"
      },
      "source": [
        "## Unsupervised Clustering - Gaussian Mixture Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gQOKaLRoF72"
      },
      "source": [
        "### Gaussian Mixtures Models in PyMC on 1-D data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAXVrnptxUTB"
      },
      "source": [
        "First we start by generating some data as a sum of 3 Gaussians, so that we know how to optimally cluster it. The Gaussian Mixture Model will have no idea about the parameters of our model. Nevertheless, visually in our input, we can see the 3 modes in our data, and the data is distributed around these modes. These are what we will call clusters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_f-pztxjX6qI"
      },
      "source": [
        "# simulate data from a known mixture distribution\n",
        "k = 3\n",
        "ndata = 200\n",
        "spread = 5\n",
        "centers = np.array([-spread, 0, spread])\n",
        "\n",
        "# simulate data from mixture distribution\n",
        "v = np.random.randint(0, k, ndata)\n",
        "data = centers[v] + np.random.randn(ndata)\n",
        "\n",
        "sns.histplot(data);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mC0lrPIjZKOj"
      },
      "source": [
        "# setup model\n",
        "model = pm.Model()\n",
        "with model:\n",
        "    # cluster sizes\n",
        "    p = pm.Dirichlet(\"p\", a=np.array([1.0, 1.0, 1.0]), shape=k)\n",
        "    # ensure all clusters have some points\n",
        "    p_min_potential = pm.Potential(\"p_min_potential\", tt.switch(tt.min(p) < 0.1, -np.inf, 0))\n",
        "    # cluster centers\n",
        "    means = pm.Normal(\"means\", mu=[0, 0, 0], sigma=15, shape=k)\n",
        "    # break symmetry\n",
        "    order_means_potential = pm.Potential(\n",
        "        \"order_means_potential\",\n",
        "        tt.switch(means[1] - means[0] < 0, -np.inf, 0)\n",
        "        + tt.switch(means[2] - means[1] < 0, -np.inf, 0),\n",
        "    )\n",
        "    # measurement error\n",
        "    sd = pm.Uniform(\"sd\", lower=0, upper=20)\n",
        "    # latent cluster of each observation\n",
        "    category = pm.Categorical(\"category\", p=p, shape=ndata)\n",
        "    # likelihood for each observed value\n",
        "    points = pm.Normal(\"obs\", mu=means[category], sigma=sd, observed=data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOI8ot62ZN_h"
      },
      "source": [
        "# fit model\n",
        "with model:\n",
        "    step1 = pm.Metropolis(vars=[p, sd, means])\n",
        "    step2 = pm.CategoricalGibbsMetropolis(vars=[category])\n",
        "    tr = pm.sample(1000, step=[step1, step2], tune=5000, return_inferencedata=False) # TODO: Change n-samples to 10000 for stable measurements"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEQIWtBHZRva"
      },
      "source": [
        "with model:\n",
        "    pm.plot_trace(tr, var_names=[\"p\", \"sd\", \"means\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2yhd-XmDYhw"
      },
      "source": [
        "### Recalling what KMeans gave us 2-D data\n",
        "\n",
        "We have already sesen the use of KMeans clustering, here we will visualize KMeans as a special case of Probabilisitic Clustering."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxKhdoAG1vs5"
      },
      "source": [
        "size, classes = 100, 4\n",
        "plt.figure(figsize=(10, 10))\n",
        "loc = np.array([[23, 25], [80, 61], [74, 13], [33, 82]])\n",
        "var = np.array([[15, 18], [21, 8], [19, 15], [13, 17]])\n",
        "\n",
        "X = np.vstack([np.stack([np.random.normal(loc=mean[0], scale=variance[1], size=size), \n",
        "                         np.random.normal(loc=mean[1], scale=variance[1], size=size)], \n",
        "                        axis=-1) for mean, variance in zip(loc, var)])\n",
        "y = np.hstack([np.full(size, fill_value=i) for i in range(classes)])\n",
        "\n",
        "def plot_kmeans(kmeans, X, n_clusters=4, rseed=0, ax=None):\n",
        "    labels = kmeans.fit_predict(X)\n",
        "\n",
        "    # plot the input data\n",
        "    ax = ax or plt.gca()\n",
        "    ax.axis('equal')\n",
        "    ax.scatter(X[:, 0], X[:, 1], c=list(map(lambda x: color_map[x], y)), s=10)\n",
        "    # plot the representation of the KMeans model\n",
        "    centers = kmeans.cluster_centers_\n",
        "    radii = [np.sqrt(np.sum((X[labels == i] - center) ** 2, axis=1).max()) for i, center in enumerate(centers)]\n",
        "    for i, (c, r) in enumerate(zip(centers, radii)):\n",
        "        ax.add_patch(plt.Circle(c, r, fc='gray', lw=3, alpha=0.2, zorder=1))\n",
        "\n",
        "kmeans = sklearn.cluster.KMeans(n_clusters=4, random_state=0)\n",
        "plot_kmeans(kmeans, X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQsbHqVMXO1z"
      },
      "source": [
        "### Applying GMMs to 2-D data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Pt-PNnVXS4i"
      },
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "def plot_gmm(gmm, X, n_clusters=4, rseed=0, ax=None):\n",
        "    labels = gmm.fit_predict(X)\n",
        "\n",
        "    # plot the input data\n",
        "    ax = ax or plt.gca()\n",
        "    ax.axis('equal')\n",
        "    ax.scatter(X[:, 0], X[:, 1], c=list(map(lambda x: color_map[x], y)), s=10)\n",
        "    # plot the representation of the gmm model\n",
        "    centers = gmm.means_\n",
        "    radii = [np.sqrt(np.sum((X[labels == i] - center) ** 2, axis=1).max()) for i, center in enumerate(centers)]\n",
        "    for i, (c, r) in enumerate(zip(centers, radii)):\n",
        "        ax.add_patch(plt.Circle(c, r, fc='gray', lw=3, alpha=0.2, zorder=1))\n",
        "\n",
        "gmm = sklearn.mixture.GaussianMixture(n_components=4, random_state=0)\n",
        "plot_gmm(gmm, X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnNX0WRJ7_EG"
      },
      "source": [
        "## Topic Modelling - Latent Dirichlet Allocation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VihySe66g6GR"
      },
      "source": [
        "### Visualizing a Dirichlet Distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haLgzqZZlq9z"
      },
      "source": [
        "import matplotlib.tri as tri\n",
        "\n",
        "corners = np.array([[0, 0], [1, 0], [0.5, 0.75**0.5]])\n",
        "AREA = 0.5 * 1 * 0.75**0.5\n",
        "triangle = tri.Triangulation(corners[:, 0], corners[:, 1])\n",
        "\n",
        "# For each corner of the triangle, the pair of other corners\n",
        "pairs = [corners[np.roll(range(3), -i)[1:]] for i in range(3)]\n",
        "# The area of the triangle formed by point xy and another pair or points\n",
        "tri_area = lambda xy, pair: 0.5 * np.linalg.norm(np.cross(*(pair - xy)))\n",
        "\n",
        "def xy2bc(xy, tol=1.e-4):\n",
        "    '''Converts 2D Cartesian coordinates to barycentric.'''\n",
        "    coords = np.array([tri_area(xy, p) for p in pairs]) / AREA\n",
        "    return np.clip(coords, tol, 1.0 - tol)\n",
        "\n",
        "class Dirichlet(object):\n",
        "    def __init__(self, alpha):\n",
        "        from math import gamma\n",
        "        from operator import mul\n",
        "        self._alpha = np.array(alpha)\n",
        "        self._coef = gamma(np.sum(self._alpha)) / \\\n",
        "                           np.multiply.reduce([gamma(a) for a in self._alpha])\n",
        "    def pdf(self, x):\n",
        "        '''Returns pdf value for `x`.'''\n",
        "        from operator import mul\n",
        "        return self._coef * np.multiply.reduce([xx ** (aa - 1)\n",
        "                                               for (xx, aa)in zip(x, self._alpha)])\n",
        "        \n",
        "def draw_pdf_contours(dist, nlevels=200, subdiv=8, **kwargs):\n",
        "    import math\n",
        "\n",
        "    refiner = tri.UniformTriRefiner(triangle)\n",
        "    trimesh = refiner.refine_triangulation(subdiv=subdiv)\n",
        "    pvals = [dist.pdf(xy2bc(xy)) for xy in zip(trimesh.x, trimesh.y)]\n",
        "\n",
        "    plt.tricontourf(trimesh, pvals, nlevels, cmap='jet', **kwargs)\n",
        "    plt.axis('equal')\n",
        "    plt.xlim(0, 1)\n",
        "    plt.ylim(0, 0.75**0.5)\n",
        "    plt.axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRkaE7Vcni0m"
      },
      "source": [
        "plt.figure(figsize=(10 * np.sqrt(3) / 2, 10))\n",
        "x_weight = 7 #@param {type: \"slider\", min: 0.0, max: 10.0}\n",
        "y_weight = 2 #@param {type: \"slider\", min: 0.0, max: 10.0}\n",
        "z_weight = 5 #@param {type: \"slider\", min: 0.0, max: 10.0}\n",
        "draw_pdf_contours(Dirichlet([x_weight, y_weight, z_weight]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UASKvVlUg-OG"
      },
      "source": [
        "### Loading in the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQy0oIRixRp0"
      },
      "source": [
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"inaugural\")\n",
        "from nltk.corpus import inaugural\n",
        "from nltk.corpus import stopwords\n",
        "stops = set(stopwords.words(\"english\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVNa_THMxtqY"
      },
      "source": [
        "data_dict = {'president': [], 'year': [], 'speech': []}\n",
        "for fileid in inaugural.fileids():\n",
        "    speech_string = \"\"\n",
        "    for word in inaugural.words(fileid):\n",
        "        word = word.lower()\n",
        "        if word not in stops and word.isalpha():\n",
        "            speech_string += word + ' '\n",
        "    data_dict[\"speech\"].append(speech_string)\n",
        "    data_dict[\"year\"].append(fileid[:4])\n",
        "    data_dict[\"president\"].append(fileid[5:-4])\n",
        "df = pd.DataFrame(data_dict)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cpdgwVThqCI"
      },
      "source": [
        "### Preprocessing and Vector generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQKkie9pH-ew"
      },
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "vocab = defaultdict(int)\n",
        "for idx, speech in enumerate(df[\"speech\"]):\n",
        "    for word in speech.split():\n",
        "        vocab[word] += 1\n",
        "vocab = np.sort(np.array(list(filter(lambda x: vocab[x] > 50, vocab.keys()))))\n",
        "word_to_index = {word: idx for idx, word in enumerate(vocab)}\n",
        "\n",
        "print(\"Constructed Vocabulary of\", len(vocab), \"words.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uB5o_4jJJBC8"
      },
      "source": [
        "speech_vectors = np.zeros(shape=(len(df), len(vocab)))\n",
        "for idx, speech in enumerate(df[\"speech\"]):\n",
        "    for word in speech.split():\n",
        "        if word in word_to_index:\n",
        "            speech_vectors[idx, word_to_index[word]] += 1\n",
        "\n",
        "print(f\"Number of words accounted for: {int(np.sum(speech_vectors))} ({100 * np.sum(speech_vectors) / np.sum(df['speech'].apply(lambda x: len(x.split())))}%)\")\n",
        "\n",
        "plt.figure(figsize=(25, 6))\n",
        "sns.heatmap(speech_vectors)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8M_qRYU9b48J"
      },
      "source": [
        "### Using SKLearn to try out LDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDUjxy_Jb8AT"
      },
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "lda = LatentDirichletAllocation(n_components=4, random_state=0)\n",
        "lda.fit(speech_vectors)\n",
        "res = lda.transform(speech_vectors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6PXCKB8fF9N"
      },
      "source": [
        "for i in range(4):\n",
        "    print(f\"Topic {i+1}:\", \" \".join([vocab[word] for word in lda.components_[i].argsort()[:5]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QM8-ibrbgBgj"
      },
      "source": [
        "df[\"topic\"] = res.argmax(axis=1)\n",
        "for i in range(4):\n",
        "    print(f\"Presidents on topic {i+1}:\", \" \".join(df[\"president\"].loc[df[\"topic\"] == i].unique()))\n",
        "print()\n",
        "df[\"topic\"] = res.argmax(axis=1)\n",
        "for i in range(4):\n",
        "    print(f\"Years on topic {i+1}:\", \" \".join(df[\"year\"].loc[df[\"topic\"] == i].unique()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pN_wrgphtEO"
      },
      "source": [
        "### Implementating Latent Dirichlet Allocation (Optional)\n",
        "\n",
        "**The specific code for LDA is not needed, and is available as a library function in SKLearn. Code sample for the same is provided in the following section**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXivnCvVLXa-"
      },
      "source": [
        "class LatentDirichletAllocation:\n",
        "    \n",
        "    def __init__(self, data, topics=10):\n",
        "        \"\"\"\n",
        "        Takes the data variable and outputs a model\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.num_topics, self.num_vocab, self.num_docs = self.data.shape[1] + 1, self.data.shape[0], topics\n",
        "        self.alpha, self.beta = np.ones(self.num_topics), np.ones(self.num_vocab)\n",
        "        with pm.Model() as model:\n",
        "            self.theta = pm.Dirichlet(\"thetas\", a=self.alpha, shape=(self.docs, self.vocab))\n",
        "            self.phi = pm.Dirichlet(\"phis\", a=self.beta, shape=(K, V))\n",
        "            self.z = pm.Categorical(\"zx\", p=thetas, shape=(W, D))\n",
        "            self.w = pm.Categorical(\"wx\", p=t.reshape(phis[z], (D*W, V)), observed=data.reshape(D*W))\n",
        "        self.model = pm.Model([self.theta, self.phi, self.z, self.w])\n",
        "        self.mcmc = pm.MCMC(self.model)\n",
        "    \n",
        "    def fit(self, iterations=1000, burn_in=10):\n",
        "        self.mcmc.sample(iterations, burn=burn_in)\n",
        "        \n",
        "    @property\n",
        "    def topics(self):\n",
        "        return self.phi.value\n",
        "\n",
        "    @property\n",
        "    def words(self):\n",
        "        return self.W.value\n",
        "    \n",
        "    @staticmethod\n",
        "    def kl_divergence(p, q):\n",
        "        return np.sum(p*np.log10(p/q))\n",
        "    \n",
        "    @staticmethod\n",
        "    def cosine_similarity(x, y):\n",
        "        return np.dot(x,y) / np.sqrt(np.dot(x,x) * np.dot(y,y))\n",
        "    \n",
        "    def sorted_docs_sim(self):\n",
        "        kldivs_docs = [(i, j, self.kl_divergence(self.theta[i].value,self.theta[j].value),\n",
        "                        self.cosine_similarity(self.data[i], self.data[j]))\n",
        "                       for i in range(len(self.theta)) for j in range(len(self.theta))\n",
        "                       if i != j]\n",
        "        return sorted(kldivs_docs, key=lambda x: x[3], reverse=True)\n",
        "    \n",
        "    def show_topic_words(self, idwords, n=10):\n",
        "        for i, t in enumerate(self.phi.value):\n",
        "            print(\"Topic %i : \" % i, \", \".join(idwords[w_] for w_ in np.argsort(t[0])[-10:] if w_ < (self.vocab-1-1)))\n",
        "    \n",
        "    def plot_words_per_topic(self, ax=None):\n",
        "        if ax is None:\n",
        "            plt.clf()\n",
        "            fig, ax = plt.subplots(1,1)\n",
        "        words = self.Z.value\n",
        "        topic_dist = dict()\n",
        "        for k_i in words:\n",
        "            for k in k_i:\n",
        "                if k not in topic_dist:\n",
        "                    topic_dist[k] = 0\n",
        "                topic_dist[k] += 1\n",
        "        ax.bar(topic_dist.keys(), topic_dist.values())\n",
        "        ax.set_xlabel(\"Topics\")\n",
        "        ax.set_ylabel(\"Counts\")\n",
        "        ax.set_title(\"Document words per topics\")\n",
        "        plt.show()\n",
        "        \n",
        "    def plot_word_dist(self, ax=None):\n",
        "        topics = self.phi.value\n",
        "        if ax is None:\n",
        "            plt.clf()\n",
        "            fig, ax = plt.subplots((len(topics)+1)/2, 2, figsize=(10,10))\n",
        "        for i, t in enumerate(topics):\n",
        "            ax[i/2][i%2].bar(range(len(t[0])), t[0])\n",
        "            ax[i/2][i%2].set_title(\"Topic %s\" % i)\n",
        "        plt.suptitle(\"Vocab word proportions per topic\")\n",
        "        fig.subplots_adjust(hspace=0.5, wspace=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lda = LatentDirichletAllocation(speech_vectors)\n",
        "lda.fit()"
      ],
      "metadata": {
        "id": "IG7tXhPRcLGB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}