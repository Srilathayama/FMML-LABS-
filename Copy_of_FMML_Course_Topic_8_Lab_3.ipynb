{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Srilathayama/FMML-LABS-/blob/main/Copy_of_FMML_Course_Topic_8_Lab_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNBGRSAl_bdz"
      },
      "source": [
        "# Bayesian Machine Learning: Lab 3 - Applying Bayes Classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftwi-r66JD-C"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/drive/1TuDbIyso6irdyBFgaHRpC-PxUICWPHcS?usp=sharing\"><img height=\"32px\" src=\"https://colab.research.google.com/img/colab_favicon.ico\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/Foundations-in-Modern-Machine-Learning/course-contents/tree/main/Bayesian/\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>\n",
        "<br>\n",
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liVAhCWf4tzE"
      },
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as pg\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51sdlLBv4yK7"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/Foundations-in-Modern-Machine-Learning/course-contents/main/Bayesian/data/arxiv-paper-data.json\n",
        "plt.style.use('ggplot')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAvMN-SCgM94"
      },
      "source": [
        "Let's make a few assumptions to come up with our first general model for classification:\n",
        "* The different features contribute equally to the result\n",
        "* The features are independent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fx1gqOFZepwF"
      },
      "source": [
        "## Bayes Classifiers in Scikit Learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzJ_W5g1ecku"
      },
      "source": [
        "### Simple 1-D Classfication using Naive Bayes in Continuous Space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3dNBKQfAlW4"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(18, 8))\n",
        "x1 = np.random.normal(1.0, 0.3, size=5000)\n",
        "x2 = np.random.normal(0.0, 0.3, size=50000)\n",
        "x = np.concatenate([x1, x2])\n",
        "y = np.concatenate([np.zeros(len(x1)), np.ones(len(x2))])\n",
        "sns.histplot(x1, color='blue', alpha=0.5, ax=ax)\n",
        "sns.histplot(x2, color='red', alpha=0.5, ax=ax)\n",
        "ax.fill_between(np.linspace(-1.0, 0.7, 100), 0, 1800,  color='r', alpha=0.1)\n",
        "ax.fill_between(np.linspace(0.7, 2.0, 100), 0, 1800,  color='b', alpha=0.1)\n",
        "ax.vlines(0.5, 0, 1750, color=\"k\", linestyles=\"--\", lw=3)\n",
        "ax.vlines(0.7, 0, 1750, color=\"g\", linestyles=\"--\", lw=3)\n",
        "ax.scatter(x1, [np.random.randint(2000, 3000) for _ in x1], color='b', s=1)\n",
        "ax.scatter(x2, [np.random.randint(2000, 3000) for _ in x2], color='r', s=1)\n",
        "ax.set_xlim(-1.0, 2.0)\n",
        "ax.set_ylim(0, 3200)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0yQ5o8ii6e0"
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "gnb = GaussianNB()\n",
        "x_reshaped = x.reshape(1, -1)\n",
        "print(x_reshaped.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZAbGgPy_iJi"
      },
      "source": [
        "## Gaussian Bayes Predictor on Predicting Flowers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yt2LCVhvLCl4"
      },
      "source": [
        "### Predictors in Scikit Learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXnkCeW4_hVM"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X = X[:, [2, 3]]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0wz1NygAElM"
      },
      "source": [
        "plt.figure(figsize=(20, 10))\n",
        "color_scheme = ['red', 'green', 'blue']\n",
        "plt.scatter(X_train[:, 0], X_train[:, 1], c=[color_scheme[idx] for idx in y_train])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSQv862RK-P8"
      },
      "source": [
        "### Visualizing the Decision Surface"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOsfUWnSb3Dv"
      },
      "source": [
        "plt.figure(figsize=(20, 10))\n",
        "X1_scan, X2_scan = np.meshgrid(np.linspace(0.5, 7.5, 250), np.linspace(0.0, 3.0, 250))\n",
        "X_scan = np.stack([np.reshape(X1_scan, -1), np.reshape(X2_scan, -1)], axis=-1)\n",
        "predictions = gnb.predict(X_scan)\n",
        "color_map = ['r', 'g', 'b']\n",
        "plt.scatter(x=X_test[:, 0], y=X_test[:, 1], c=[color_map[x] for x in y_test])\n",
        "sns.kdeplot(x=X_scan[:, 0], y=X_scan[:, 1], hue=[color_map[x] for x in predictions], fill=False)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QubPkrx1_Y7A"
      },
      "source": [
        "## Bayes on Text - Classifying papers at Arxiv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCSW0RdiPdn7"
      },
      "source": [
        "### Data Cleaning and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUw38arxIwcJ"
      },
      "source": [
        "arxiv_df = pd.read_json(open(\"arxiv-paper-data.json\", 'r'))\n",
        "arxiv_df[\"tag\"] = arxiv_df[\"tag\"].apply(lambda x: json.loads(x.replace(\"\\'\", \"\\\"\").replace(\"None\", \"\\\"None\\\"\"))[0]['term'].split('.')[0])\n",
        "arxiv_df[\"tag\"] = arxiv_df[\"tag\"].apply(lambda x: 'physics' if x in ['physics', 'quant-ph', 'astro-ph', 'hep-ex', 'hep-ph', 'hep-lat', 'hep-th', 'nucl-th'] else x)\n",
        "arxiv_df = arxiv_df.drop([\"day\", \"month\", \"link\", \"author\"], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTmJprNco60K"
      },
      "source": [
        "SAMPLES_COMP = 3500 #@param {type: \"slider\", min: 1000, max: 4500}\n",
        "SAMPLES_STAT = 2500 #@param {type: \"slider\", min: 1000, max: 4500}\n",
        "WORD_COUNT_THRESHOLD = 1  #@param {type: \"slider\", min: 1, max: 500}\n",
        "WORD_LENGTH_THRESHOLD = 8  #@param {type: \"slider\", min: 4, max: 20}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2HJrOm_Ow5j"
      },
      "source": [
        "df = pd.concat([arxiv_df[arxiv_df['tag'] == 'cs'].sample(SAMPLES_COMP, replace=False), \n",
        "                arxiv_df[arxiv_df['tag'] == 'stat'].sample(SAMPLES_STAT, replace=False)]).sample(frac=1)\n",
        "df['keywords'] = df.apply(lambda x: [word.lower() for word in re.sub('[^(\\w| )]', '', re.sub('(-|_|\\n)', ' ', x.summary + ' ' + x.title)).split() \n",
        "                                     if len(word) > WORD_LENGTH_THRESHOLD and not re.search('(\\d|\\(|\\))', word)], axis=1)\n",
        "train_df, test_df = df.iloc[:int(len(df) * 0.8)], df.iloc[int(len(df) * 0.8):]\n",
        "\n",
        "train_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTmwDtpml3PY"
      },
      "source": [
        "df['keywords']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46mYKPL-PkUe"
      },
      "source": [
        "### Learning Probability Distributions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hp32ePNGqgN_"
      },
      "source": [
        "from collections import defaultdict\n",
        "dictionary = defaultdict(lambda: [0, 0])\n",
        "for desc, tag in zip(df['keywords'], df['tag']):\n",
        "    for word in set(desc):\n",
        "        if tag == 'cs':\n",
        "            dictionary[word][0] += 1\n",
        "        else:\n",
        "            dictionary[word][1] += 1\n",
        "\n",
        "print(\"We have a dictionary of\", len(dictionary), \"words.\")\n",
        "\n",
        "words, counts_cs, counts_stat = [], [], []\n",
        "for word, count in dictionary.items():\n",
        "    if count[0] + count[1] > WORD_COUNT_THRESHOLD:\n",
        "        words.append(word)\n",
        "        counts_cs.append(count[0])\n",
        "        counts_stat.append(count[1])\n",
        "print(\"Out of those\", len(words), f\"have over {WORD_COUNT_THRESHOLD} occurances\")\n",
        "\n",
        "fig = pg.Figure([pg.Bar(x=words[:200], y=counts_cs[:200]), \n",
        "                 pg.Bar(x=words[:200], y=counts_stat[:200])])\n",
        "fig.update_layout(title_text=f'Some random words and the frequency plot of those with over {WORD_COUNT_THRESHOLD} occurances')\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce8ZKif4oVQQ"
      },
      "source": [
        "For each word, for both the subjects, we can apply Bayes rule. \n",
        "\n",
        "$$P\\bigg(\\frac{\\text{subject}}{\\text{word}}\\bigg) = P\\bigg(\\frac{\\text{word}}{\\text{subject}}\\bigg) \\times \\frac{P(\\text{subject})}{P(\\text{word})}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDMMGwWIPqX_"
      },
      "source": [
        "### Computing the resultant probabilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QEyxv7yoOwP"
      },
      "source": [
        "prior = np.array([train_df[\"tag\"].value_counts()[\"cs\"], train_df[\"tag\"].value_counts()[\"stat\"]]) / len(train_df)\n",
        "evidence = np.array([counts_cs / train_df[\"tag\"].value_counts()[\"cs\"], counts_stat / train_df[\"tag\"].value_counts()[\"stat\"]]).T\n",
        "likelihood = (np.array(counts_cs) + np.array(counts_stat)) / len(train_df)\n",
        "\n",
        "prior.shape, likelihood.shape, evidence.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXbrRsLNtc_B"
      },
      "source": [
        "words_to_index = defaultdict(lambda: -1)\n",
        "for idx, word in enumerate(words):\n",
        "    words_to_index[word] = idx\n",
        "\n",
        "correct, total = 0, 0\n",
        "\n",
        "answers = []\n",
        "for tag, keywords in zip(test_df[\"tag\"], test_df[\"keywords\"]):\n",
        "    this_counts = np.zeros(len(words))\n",
        "    for word in keywords:\n",
        "        idx = words_to_index[word]\n",
        "        if idx != -1:\n",
        "            this_counts[idx] = 1\n",
        "\n",
        "    marginal = np.multiply(prior.T, np.divide(\n",
        "        np.multiply(this_counts.reshape(-1, 1), evidence), likelihood.reshape(-1, 1))) + 0.001\n",
        "    joint = np.sum(np.log(marginal), axis=0)\n",
        "    predicted = 'cs' if joint[0] > joint[1] else 'stat'\n",
        "    answers.append(predicted)\n",
        "    if predicted == tag:\n",
        "        correct += 1\n",
        "    total += 1\n",
        "test_df.insert(1, \"predictions\", answers, True)\n",
        "\n",
        "print(\"Accuracy:\", correct / total)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyGh4gqSPtwl"
      },
      "source": [
        "### Visualizing a few examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3xWaDKW_wFU"
      },
      "source": [
        "def explain(example):\n",
        "    from IPython.display import HTML\n",
        "    title, desc, answer = example[\"title\"], example[\"summary\"], example[\"tag\"]\n",
        "    html = \"\"\n",
        "    title = re.sub(' +', ' ', re.sub('[^(\\w| )]', '', re.sub('(-|_|\\n)', ' ', title)))\n",
        "    desc = re.sub(' +', ' ', re.sub('[^(\\w| )]', '', re.sub('(-|_|\\n)', ' ', desc)))\n",
        "    for text in [title, desc]:\n",
        "        for word in text.split():\n",
        "            idx = words_to_index[word.lower()]\n",
        "            if idx == -1:\n",
        "                html += f\"<span style=''>{word}</span> \"\n",
        "            else:\n",
        "                posterior = np.multiply(evidence[idx], prior)\n",
        "                posterior /= np.sum(posterior)\n",
        "                if posterior[0] > posterior[1] if answer == 'cs' else (posterior[0] <= posterior[1]):\n",
        "                    color = max(0.3, posterior[0] - posterior[1])\n",
        "                    html += f\"<span style='background-color:rgba(0, 255, 0, {color});'>{word}</span> \"\n",
        "                else:\n",
        "                    color = max(0.3, posterior[1] - posterior[0])\n",
        "                    html += f\"<span style='background-color:rgba(255, 0, 0, {color});'>{word}</span> \"\n",
        "        html += \"<br/><br/>\"\n",
        "    return HTML(html)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ws3I1_XINubR"
      },
      "source": [
        "x = test_df[np.logical_and(test_df[\"predictions\"] == test_df[\"tag\"], test_df[\"tag\"] == \"cs\")].iloc[0]\n",
        "explain(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkU9IpatOrSi"
      },
      "source": [
        "x = test_df[np.logical_and(test_df[\"predictions\"] == test_df[\"tag\"], test_df[\"tag\"] == \"stat\")].iloc[0]\n",
        "explain(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUEgAIY9OuFm"
      },
      "source": [
        "x = test_df[np.logical_and(test_df[\"predictions\"] != test_df[\"tag\"], test_df[\"tag\"] == \"cs\")].iloc[0]\n",
        "explain(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LrdKBK3OvSa"
      },
      "source": [
        "x = test_df[np.logical_and(test_df[\"predictions\"] != test_df[\"tag\"], test_df[\"tag\"] == \"stat\")].iloc[0]\n",
        "explain(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMu503ojWUNy"
      },
      "source": [
        "### Visualizing the Learnt Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxoqWikbWafA"
      },
      "source": [
        "print(\"Some Computer Science words are:\")\n",
        "for word_cs in np.argsort(evidence[:, 0] - evidence[:, 1])[-10:]:\n",
        "    print(f\"\\t{words[word_cs]} ({evidence[word_cs, 0]})\")\n",
        "print()\n",
        "print(\"Some Statistics words are:\")\n",
        "for word_st in np.argsort(evidence[:, 1] - evidence[:, 0])[-10:]:\n",
        "    print(f\"\\t{words[word_st]} ({evidence[word_st, 1]})\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzemqaNmD5Iu"
      },
      "source": [
        "## Probability Smoothing\n",
        "\n",
        "Probability smoothing is a language modeling technique that assigns some non-zero probability to events that were unseen in the training data. This has the effect that the probability mass is divided over more events, hence the probability distribution becomes more smooth."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qls2mkAULnOF"
      },
      "source": [
        "### Realizing the Problem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SlsiUp-Gha3"
      },
      "source": [
        "posterior = np.multiply(np.divide(evidence, np.reshape(likelihood, (-1, 1))), np.reshape(prior, (1, -1)))\n",
        "\n",
        "fig = pg.Figure([pg.Bar(x=words, y=evidence[:20, 0]), \n",
        "                 pg.Bar(x=words, y=posterior[:20, 0])])\n",
        "fig.update_layout(title_text='Evidence and Posterior for each word')\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmR91QL_JktJ"
      },
      "source": [
        "print(\"Some words which never appear in Computer Science paper:\\n\\t\" + \"\\n\\t\".join([words[idx] for idx in np.where(np.array(counts_cs) == 0)[0][:20]]))\n",
        "print(\"Some words which never appear in Statistics papers:\\n\\t\" + \"\\n\\t\".join([words[idx] for idx in np.where(np.array(counts_stat) == 0)[0][:20]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqtLwpJuFXts"
      },
      "source": [
        "Notice the fact that there are cases where the Evidence $P\\big(\\frac{\\text{word}}{\\text{subject}}\\big) \\rightarrow 0$. This is because there are occurances of the given word in the corpus (all the abstract of papers put together) of that subject.\n",
        "\n",
        "However, this is no way implies that if a word (eg. Employment) never occurs in our training corpus in a Computer Science paper, it will also never occur in a paper in the test set.\n",
        "\n",
        "Therefore, setting $P(\\text{word} | \\text{subject}) = 0$ is incorrect, since it will make the posterior $P(\\text{subject} | \\text{word}) = 0$, even if the corresponding count it 0, we need to come up with a smoother formulation of the Evidence variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eboxp4LpMIG9"
      },
      "source": [
        "### Laplace Smoothing\n",
        "\n",
        "\n",
        "\n",
        "https://towardsdatascience.com/laplace-smoothing-in-na%C3%AFve-bayes-algorithm-9c237a8bdece\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZvwy4isMdAK"
      },
      "source": [
        "alpha = 0.1\n",
        "counts_cs, counts_stat = np.array(counts_cs), np.array(counts_stat)\n",
        "evidence_smooth = np.array([(counts_cs + alpha) / (train_df[\"tag\"].value_counts()[\"cs\"] + alpha * len(words)),\n",
        "                            (counts_stat + alpha) / (train_df[\"tag\"].value_counts()[\"stat\"] + alpha * len(words))]).T\n",
        "print(np.min(evidence_smooth))\n",
        "sns.displot(evidence_smooth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkYKtRsCPh-0"
      },
      "source": [
        "words_to_index = defaultdict(lambda: -1)\n",
        "for idx, word in enumerate(words):\n",
        "    words_to_index[word] = idx\n",
        "\n",
        "correct, total = 0, 0\n",
        "\n",
        "answers = []\n",
        "for tag, keywords in zip(test_df[\"tag\"], test_df[\"keywords\"]):\n",
        "    this_counts = np.zeros(len(words))\n",
        "    for word in keywords:\n",
        "        idx = words_to_index[word]\n",
        "        if idx != -1:\n",
        "            this_counts[idx] = 1\n",
        "\n",
        "    marginal = np.multiply(prior.T, np.divide(np.multiply(this_counts.reshape(-1, 1), evidence_smooth), likelihood.reshape(-1, 1))) + 0.0000001\n",
        "    joint = np.sum(np.log(marginal), axis=0)\n",
        "    predicted = 'cs' if joint[0] > joint[1] else 'stat'\n",
        "    answers.append(predicted)\n",
        "    if predicted == tag:\n",
        "        correct += 1\n",
        "    total += 1\n",
        "test_df.insert(1, \"predictions\", answers, True)\n",
        "\n",
        "print(\"Accuracy:\", correct / total)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}